{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08d6c339",
   "metadata": {},
   "source": [
    "**线性回归**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d387391f",
   "metadata": {},
   "source": [
    "$$\n",
    "h_\\alpha(x) = \\alpha_0 + \\alpha_1x_1 + \\alpha_2x_2 (\\alpha_0是偏置项) \\\\\n",
    "h_\\alpha(x) = \\alpha_0x_0 + \\alpha_1x_1 + \\alpha_2x_2 (x_0是全1列，经常会在 \\\\数据处理中看到，为了拼接成矩阵计算) \\\\\n",
    "h_\\alpha(x) = \\sum_{i=0}^n\\frac{\\alpha_i}{x_i} = \\alpha^Tx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bbff52",
   "metadata": {},
   "source": [
    "**找到最合适的一条线（想象一个高维）来最好的拟合我们的数据点**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f64388",
   "metadata": {},
   "source": [
    "**真实值和预测值之间肯定是要存在差异的(用𝛽来表示误差)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4754285",
   "metadata": {},
   "source": [
    "$$\n",
    "y^i = \\alpha^Tx^i + \\beta^i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b75d3d",
   "metadata": {},
   "source": [
    "**误差$\\beta^i$是独立并且具有相同的分布，并且服从均值为0方差为$𝛼^2$    \n",
    "独立：张三李四一起来贷款，他两没关系    \n",
    "同分布：他两都来自同一家银行    \n",
    "高斯分布：银行可能会多给，也可能会少给，但绝大多数情况下这个浮动不会太大，    \n",
    "极少情况下浮动会比较大，符合正常情况**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdae90f",
   "metadata": {},
   "source": [
    "**误差高斯分布**    \n",
    "$$\n",
    "p(\\beta_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(\\beta_i)^2}{2\\sigma^2})\n",
    "$$\n",
    "**带入误差公式求𝛼**\n",
    "$$\n",
    "p(y_i|x_i;\\alpha) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^i -\\alpha^Tx^i)^2}{2\\sigma^2})\n",
    "$$\n",
    "**似然函数：什么样的参数跟我们的数据组合恰好是真实值    \n",
    "独立同分布联合概率密度等于边缘概率密度的累乘**\n",
    "$$\n",
    "L(\\alpha) = \\prod_{m}^{i=1}p(y_i|x_i;\\alpha)\n",
    "$$\n",
    "**对数似然：乘法难解，加法容易，对数乘法变成对数加法    \n",
    "求解的结果变了，但极值点不变，我们需要的是极值点，对结果不关心**\n",
    "$$\n",
    "logL(\\alpha) = log\\prod_{m}^{i=1}p(y_i|x_i;\\alpha)\n",
    "$$\n",
    "**展开化简**\n",
    "$$\n",
    "logL(\\alpha) = \\sum_{m}^{i=1}log\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^i -\\alpha^Tx^i)^2}{2\\sigma^2}) \\\\\n",
    "= mlog\\frac{1}{\\sqrt{2\\pi}\\sigma}-\\frac{1}{(\\sigma)^2}*\\frac{1}{2}\\sum_{m}^{i=1}(y^i -\\alpha^Tx^i)^2\n",
    "$$\n",
    "**需要让似然函数整体的值越大越好，所以J(𝛼)越小越好**\n",
    "$$\n",
    "J(\\alpha) = \\frac{1}{2}\\sum_{m}^{i=1}(y^i -\\alpha^Tx^i)^2（最小二乘法）\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76854aab",
   "metadata": {},
   "source": [
    "**目标函数**\n",
    "$$\n",
    "J(\\alpha) = \\frac{1}{2}\\sum_{m}^{i=1}(h_\\alpha(x_i)-y_i)^2 = \\frac{1}{2}(X\\alpha-y)^T(X\\alpha-y)\n",
    "$$\n",
    "**求偏导(求极值点)**\n",
    "$$\n",
    "\\nabla_{\\alpha}J(\\alpha) = \\nabla_{\\alpha}\\lgroup\\frac{1}{2}(X\\alpha-y)^T(X\\alpha-y)\\rgroup \\\\\n",
    "=\\nabla_{\\alpha}\\lgroup\\frac{1}{2}(\\alpha^TX^T-y^T)(X\\alpha-y)\\rgroup \\\\\n",
    "=\\nabla_{\\alpha}\\lgroup\\frac{1}{2}(\\alpha^TX^TX\\alpha-\\alpha^TX^Ty-y^TX\\alpha+y^Ty)\\rgroup \\\\\n",
    "=\\frac{1}{2}(2X^TX\\alpha-X^Ty-(y^TX)^T) \\\\\n",
    "=X^TX\\alpha-X^Ty\n",
    "$$\n",
    "**偏导等于0时为极值点**\n",
    "$$\n",
    "\\alpha = (X^TX)^{-1}X^Ty（求逆的过程不一定成立）\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9979dcd4",
   "metadata": {},
   "source": [
    "**梯度下降    \n",
    "当我们得到了一个目标函数后，如何进行求解？直接求解？（并不一定可解）    \n",
    "常规套路：交给机器一堆数据，然后告诉他什么样的学习方式是对的（目标函数），然后让他朝着  这个方向去做    \n",
    "如何优化：每次优化一点点，一步步完成迭代    \n",
    "步骤：    \n",
    "找到当前最合适的方向（每个参数分别求偏导）    \n",
    "步伐要小    \n",
    "按照方向和步伐去更新参数    \n",
    "方式：    \n",
    "目标函数$J(\\alpha) = \\frac{1}{2m}\\sum_{m}^{i=1}(y^i -h_\\alpha(x^i))^2$    \n",
    "批量梯度下降（容易得到最优解，考虑所有样本，速度慢）  \n",
    "$$\n",
    "\\frac{\\partial J(\\alpha)}{\\partial \\alpha_j} = -\\frac{1}{m}\\sum_{m}^{i=1}(y^i -h_\\alpha(x^i))x_j^i \\\\\n",
    "\\alpha_j = \\alpha_j + \\frac{1}{m}\\sum_{m}^{i=1}(y^i -h_\\alpha(x^i))x_j^i\n",
    "$$\n",
    "随机梯度下降（随机下降，不一定会是正确的方向，速度快） \n",
    "$$\n",
    "\\alpha_j = \\alpha_j + \\frac{1}{m}\\sum_{m}^{i=1}(y^i -h_\\alpha(x^i))x_j^i\n",
    "$$\n",
    "小批量梯度下降（每次选择更新一小部分数据，batch一次迭代的样本量，batch大慢精确，batch小快粗略，设备允许的情况下越大越好）    \n",
    "$$\n",
    "\\alpha_j = \\alpha_j - learningrate*\\frac{1}{10}\\sum_{i+9}^{k=i}(y^k -h_\\alpha(x^k))x_j^k\n",
    "$$\n",
    "学习率（沿着这个方向走多大的距离）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c559c11a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
